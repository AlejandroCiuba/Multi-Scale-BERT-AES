2024-11-16 16:04:12 : ===================== 2024-11-16T16:04:12.807195 =====================
2024-11-16 16:04:12 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:04:12 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:11:52 : ===================== 2024-11-16T16:11:52.997775 =====================
2024-11-16 16:11:52 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:11:52 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:12:00 : Training Set Size: 32
2024-11-16 16:12:00 : Started training loop for p3.pt
2024-11-16 16:12:04 : tensor([    nan,     nan, 29.2988, 33.2335,     nan,     nan,     nan, 24.7811,
            nan,     nan,     nan,     nan, 30.7154,     nan,     nan,     nan,
            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
        28.5809,     nan,     nan,     nan,     nan,     nan,     nan,     nan],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:12:07 : 0/80 | 0/2: nan
2024-11-16 16:31:44 : ===================== 2024-11-16T16:31:44.078633 =====================
2024-11-16 16:31:44 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:31:44 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:31:46 : Training Set Size: 32
2024-11-16 16:31:46 : Started training loop for p3.pt
2024-11-16 16:31:49 : tensor([    nan,     nan, 28.6251, 33.2736,     nan,     nan,     nan, 24.6219,
            nan,     nan,     nan,     nan, 31.7434,     nan,     nan,     nan,
            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
        29.3802,     nan,     nan,     nan,     nan,     nan,     nan,     nan],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:31:51 : 0/80 | 0/2: nan
2024-11-16 16:35:58 : ===================== 2024-11-16T16:35:58.998763 =====================
2024-11-16 16:35:58 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:35:58 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:36:00 : Training Set Size: 32
2024-11-16 16:36:00 : Started training loop for p3.pt
2024-11-16 16:36:03 : tensor([    nan,     nan, 29.4686, 33.2220,     nan,     nan,     nan, 24.3826,
            nan,     nan,     nan,     nan, 31.4093,     nan,     nan,     nan,
            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
        28.7828,     nan,     nan,     nan,     nan,     nan,     nan,     nan],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:36:05 : 0/80 | 0/2: nan
2024-11-16 16:39:05 : ===================== 2024-11-16T16:39:05.820708 =====================
2024-11-16 16:39:05 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:39:05 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:39:07 : Training Set Size: 32
2024-11-16 16:39:07 : Started training loop for p3.pt
2024-11-16 16:39:10 : tensor([    nan,     nan, 28.8246, 33.4817,     nan,     nan,     nan, 24.6246,
            nan,     nan,     nan,     nan, 31.4633,     nan,     nan,     nan,
            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
        29.4130,     nan,     nan,     nan,     nan,     nan,     nan,     nan],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:39:12 : 0/80 | 0/2: nan
2024-11-16 16:42:21 : ===================== 2024-11-16T16:42:21.314638 =====================
2024-11-16 16:42:21 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:42:21 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:42:22 : Training Set Size: 32
2024-11-16 16:42:22 : Started training loop for p3.pt
2024-11-16 16:42:26 : tensor([    nan,     nan, 28.8228, 33.3558,     nan,     nan,     nan, 24.2734,
            nan,     nan,     nan,     nan, 30.5493,     nan,     nan,     nan,
            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
        28.6163,     nan,     nan,     nan,     nan,     nan,     nan,     nan],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:42:28 : 0/80 | 0/2: nan
2024-11-16 16:45:17 : ===================== 2024-11-16T16:45:17.079381 =====================
2024-11-16 16:45:17 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:45:17 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:45:19 : Training Set Size: 32
2024-11-16 16:45:19 : Started training loop for p3.pt
2024-11-16 16:45:22 : tensor([    nan,     nan, 28.6497, 33.3565,     nan,     nan,     nan, 24.1443,
            nan,     nan,     nan,     nan, 31.2488,     nan,     nan,     nan,
            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
        29.2669,     nan,     nan,     nan,     nan,     nan,     nan,     nan],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:45:24 : 0/80 | 0/2: nan
2024-11-16 16:47:21 : ===================== 2024-11-16T16:47:21.362947 =====================
2024-11-16 16:47:21 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:47:21 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:47:22 : Training Set Size: 32
2024-11-16 16:47:22 : Started training loop for p3.pt
2024-11-16 16:47:25 : tensor([    nan,     nan, 28.2742, 33.3336,     nan,     nan,     nan, 24.1638,
            nan,     nan,     nan,     nan, 30.0302,     nan,     nan,     nan,
            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
        29.4031,     nan,     nan,     nan,     nan,     nan,     nan,     nan],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:47:27 : 0/80 | 0/2: nan
2024-11-16 16:50:23 : ===================== 2024-11-16T16:50:23.094890 =====================
2024-11-16 16:50:23 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:50:23 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:50:24 : Training Set Size: 32
2024-11-16 16:50:24 : Started training loop for p3.pt
2024-11-16 16:50:27 : tensor([ 4.8119,  3.5907, 29.6426, 33.6305,  4.6131,  3.2314,  8.9508, 24.9959,
         9.7163, 24.1468,  5.0991,  3.4674, 31.2208,  4.1425,  4.4669,  2.9757,
         4.4072,  4.1979,  5.9295,  7.7084,  8.9065,  8.7453,  3.1876,  4.1109,
        28.8759,  2.2085,  6.6125, 21.0265,  4.1892, 23.3967,  4.0763, 22.3466],
       device='cuda:0', grad_fn=<AddBackward0>)
2024-11-16 16:50:29 : 0/80 | 0/2: 89.94213
2024-11-16 16:52:36 : ===================== 2024-11-16T16:52:36.043097 =====================
2024-11-16 16:52:36 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:52:36 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=128, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:52:37 : Training Set Size: 128
2024-11-16 16:52:37 : Started training loop for p3.pt
2024-11-16 16:52:42 : 0/80 | 0/5: 87.97707
2024-11-16 16:55:13 : ===================== 2024-11-16T16:55:13.207472 =====================
2024-11-16 16:55:13 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:55:13 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=128, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:55:14 : Training Set Size: 128
2024-11-16 16:55:14 : Started training loop for p3.pt
2024-11-16 16:55:19 : 0/80 | 0/5: 88.90764
2024-11-16 16:55:23 : 0/80 | 1/5: 0.88150
2024-11-16 16:55:27 : 0/80 | 2/5: 2.17681
2024-11-16 16:55:31 : 0/80 | 3/5: 1.64284
2024-11-16 16:58:59 : ===================== 2024-11-16T16:58:59.533668 =====================
2024-11-16 16:58:59 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 16:58:59 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=128, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 16:59:00 : Training Set Size: 128
2024-11-16 16:59:00 : Started training loop for p3.pt
2024-11-16 16:59:05 : 0/80 | 0/5: 88.83038
2024-11-16 16:59:09 : 0/80 | 1/5: 0.92548
2024-11-16 16:59:13 : 0/80 | 2/5: 2.39812
2024-11-16 16:59:17 : 0/80 | 3/5: 1.71249
2024-11-16 17:02:06 : ===================== 2024-11-16T17:02:06.736064 =====================
2024-11-16 17:02:06 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 17:02:06 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 17:02:08 : Training Set Size: 32
2024-11-16 17:02:08 : Started training loop for p3.pt
2024-11-16 17:02:13 : ===================== 2024-11-16T17:02:13.188337 =====================
2024-11-16 17:02:13 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 17:02:13 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=128, epochs=80, sample=32, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 17:02:13 : 0/80 | 0/2: 89.44585
2024-11-16 17:02:14 : Training Set Size: 32
2024-11-16 17:02:14 : Started training loop for p3.pt
2024-11-16 17:02:19 : 0/80 | 0/1: 89.21426
2024-11-16 17:07:46 : ===================== 2024-11-16T17:07:46.710836 =====================
2024-11-16 17:07:46 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 17:07:46 : Hyperparameters: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=2, sample=-1, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 17:07:48 : Training Set Size: 1055
2024-11-16 17:07:48 : Started training loop for p3.pt
2024-11-16 17:07:53 : 0/2 | 0/33: 92.39171
2024-11-16 17:07:57 : 0/2 | 1/33: 0.80461
2024-11-16 17:08:00 : 0/2 | 2/33: 1.89468
2024-11-16 17:08:04 : 0/2 | 3/33: 1.86927
2024-11-16 17:08:08 : 0/2 | 4/33: 1.29136
2024-11-16 17:08:12 : 0/2 | 5/33: 0.95591
2024-11-16 17:08:16 : 0/2 | 6/33: 0.65024
2024-11-16 17:08:20 : 0/2 | 7/33: 0.42479
2024-11-16 17:08:24 : 0/2 | 8/33: 0.31937
2024-11-16 17:08:28 : 0/2 | 9/33: 0.33240
2024-11-16 17:08:31 : 0/2 | 10/33: 0.32845
2024-11-16 17:08:35 : 0/2 | 11/33: 0.37701
2024-11-16 17:08:39 : 0/2 | 12/33: 0.36360
2024-11-16 17:08:43 : 0/2 | 13/33: 0.45805
2024-11-16 17:08:47 : 0/2 | 14/33: 0.42820
2024-11-16 17:08:51 : 0/2 | 15/33: 0.38573
2024-11-16 17:08:55 : 0/2 | 16/33: 0.38221
2024-11-16 17:08:59 : 0/2 | 17/33: 0.35471
2024-11-16 17:09:02 : 0/2 | 18/33: 0.32104
2024-11-16 17:09:06 : 0/2 | 19/33: 0.29701
2024-11-16 17:09:10 : 0/2 | 20/33: 0.27544
2024-11-16 17:09:14 : 0/2 | 21/33: 0.25706
2024-11-16 17:09:18 : 0/2 | 22/33: 0.21668
2024-11-16 17:09:22 : 0/2 | 23/33: 0.21360
2024-11-16 17:09:26 : 0/2 | 24/33: 0.23551
2024-11-16 17:09:30 : 0/2 | 25/33: 0.20451
2024-11-16 17:09:34 : 0/2 | 26/33: 0.20067
2024-11-16 17:09:38 : 0/2 | 27/33: 0.18484
2024-11-16 17:09:41 : 0/2 | 28/33: 0.20532
2024-11-16 17:09:45 : 0/2 | 29/33: 0.21758
2024-11-16 17:09:49 : 0/2 | 30/33: 0.22101
2024-11-16 17:09:53 : 0/2 | 31/33: 0.21338
2024-11-16 17:09:57 : 0/2 | 32/33: 0.18637
2024-11-16 17:10:28 : Evaluation loss at epoch 0: 0.25350
2024-11-16 17:10:32 : 1/2 | 0/33: 0.27699
2024-11-16 17:10:36 : 1/2 | 1/33: 0.30373
2024-11-16 17:10:40 : 1/2 | 2/33: 0.28893
2024-11-16 17:10:44 : 1/2 | 3/33: 0.30433
2024-11-16 17:10:48 : 1/2 | 4/33: 0.24835
2024-11-16 17:10:52 : 1/2 | 5/33: 0.20045
2024-11-16 17:10:56 : 1/2 | 6/33: 0.15628
2024-11-16 17:11:00 : 1/2 | 7/33: 0.18419
2024-11-16 17:11:04 : 1/2 | 8/33: 0.14196
2024-11-16 17:11:08 : 1/2 | 9/33: 0.13891
2024-11-16 17:11:12 : 1/2 | 10/33: 0.10620
2024-11-16 17:11:16 : 1/2 | 11/33: 0.08688
2024-11-16 17:11:20 : 1/2 | 12/33: 0.10075
2024-11-16 17:11:24 : 1/2 | 13/33: 0.08294
2024-11-16 17:11:28 : 1/2 | 14/33: 0.09320
2024-11-16 17:11:32 : 1/2 | 15/33: 0.08476
2024-11-16 17:11:36 : 1/2 | 16/33: 0.07846
2024-11-16 17:11:40 : 1/2 | 17/33: 0.09895
2024-11-16 17:11:44 : 1/2 | 18/33: 0.08165
2024-11-16 17:11:48 : 1/2 | 19/33: 0.09500
2024-11-16 17:11:52 : 1/2 | 20/33: 0.06128
2024-11-16 17:11:56 : 1/2 | 21/33: 0.09478
2024-11-16 17:12:01 : 1/2 | 22/33: 0.09404
2024-11-16 17:12:05 : 1/2 | 23/33: 0.07796
2024-11-16 17:12:09 : 1/2 | 24/33: 0.06681
2024-11-16 17:12:13 : 1/2 | 25/33: 0.07779
2024-11-16 17:12:17 : 1/2 | 26/33: 0.07038
2024-11-16 17:12:21 : 1/2 | 27/33: 0.08966
2024-11-16 17:12:25 : 1/2 | 28/33: 0.09874
2024-11-16 17:12:29 : 1/2 | 29/33: 0.10751
2024-11-16 17:12:33 : 1/2 | 30/33: 0.12178
2024-11-16 17:12:37 : 1/2 | 31/33: 0.11554
2024-11-16 17:12:41 : 1/2 | 32/33: 0.10024
2024-11-16 17:13:11 : Evaluation loss at epoch 1: 0.19242
2024-11-16 17:13:11 : Performing evaluation on the test set
2024-11-16 17:13:26 : Fine-tuning complete on prompt 3
2024-11-16 18:26:18 : ===================== 2024-11-16T18:26:18.217825 =====================
2024-11-16 18:26:18 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 18:26:18 : Settings: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=2, sample=-1, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 18:26:27 : Train/Eval/Test: 1055/336/335
2024-11-16 18:26:27 : Started training loop for p3.pt
2024-11-16 18:26:35 : 0/2 | 0/33: 94.13527
2024-11-16 18:26:39 : 0/2 | 1/33: 0.75408
2024-11-16 18:26:43 : 0/2 | 2/33: 1.88693
2024-11-16 18:26:47 : 0/2 | 3/33: 1.62419
2024-11-16 18:26:51 : 0/2 | 4/33: 1.40406
2024-11-16 18:26:55 : 0/2 | 5/33: 1.01510
2024-11-16 18:26:59 : 0/2 | 6/33: 0.75076
2024-11-16 18:27:03 : 0/2 | 7/33: 0.44485
2024-11-16 18:27:07 : 0/2 | 8/33: 0.34074
2024-11-16 18:27:10 : 0/2 | 9/33: 0.31272
2024-11-16 18:27:14 : 0/2 | 10/33: 0.32310
2024-11-16 18:27:18 : 0/2 | 11/33: 0.33680
2024-11-16 18:27:22 : 0/2 | 12/33: 0.38192
2024-11-16 18:27:26 : 0/2 | 13/33: 0.41448
2024-11-16 18:27:30 : 0/2 | 14/33: 0.42603
2024-11-16 18:27:34 : 0/2 | 15/33: 0.40422
2024-11-16 18:27:38 : 0/2 | 16/33: 0.40293
2024-11-16 18:27:42 : 0/2 | 17/33: 0.34632
2024-11-16 18:27:46 : 0/2 | 18/33: 0.32085
2024-11-16 18:27:50 : 0/2 | 19/33: 0.32253
2024-11-16 18:27:54 : 0/2 | 20/33: 0.25003
2024-11-16 18:27:58 : 0/2 | 21/33: 0.23807
2024-11-16 18:28:01 : 0/2 | 22/33: 0.21572
2024-11-16 18:28:05 : 0/2 | 23/33: 0.23378
2024-11-16 18:28:09 : 0/2 | 24/33: 0.20245
2024-11-16 18:28:13 : 0/2 | 25/33: 0.20078
2024-11-16 18:28:17 : 0/2 | 26/33: 0.20106
2024-11-16 18:28:21 : 0/2 | 27/33: 0.19199
2024-11-16 18:28:25 : 0/2 | 28/33: 0.20469
2024-11-16 18:28:29 : 0/2 | 29/33: 0.21577
2024-11-16 18:28:33 : 0/2 | 30/33: 0.22158
2024-11-16 18:28:37 : 0/2 | 31/33: 0.18835
2024-11-16 18:28:41 : 0/2 | 32/33: 0.20108
2024-11-16 18:29:12 : 0: Evaluation loss | Pearson | qwk : 0.24772 | 0.70800 | 0.48400
2024-11-16 18:29:12 : Saving model p3.pt on epoch 0 (0.24772 is the new best loss!)
2024-11-16 18:29:18 : 1/2 | 0/33: 0.27954
2024-11-16 18:29:22 : 1/2 | 1/33: 0.29595
2024-11-16 18:29:26 : 1/2 | 2/33: 0.28333
2024-11-16 18:29:30 : 1/2 | 3/33: 0.26657
2024-11-16 18:29:34 : 1/2 | 4/33: 0.23761
2024-11-16 18:29:39 : 1/2 | 5/33: 0.16557
2024-11-16 18:29:43 : 1/2 | 6/33: 0.13787
2024-11-16 18:29:47 : 1/2 | 7/33: 0.13865
2024-11-16 18:29:51 : 1/2 | 8/33: 0.11414
2024-11-16 18:29:55 : 1/2 | 9/33: 0.11811
2024-11-16 18:29:59 : 1/2 | 10/33: 0.08908
2024-11-16 18:30:03 : 1/2 | 11/33: 0.11822
2024-11-16 18:30:07 : 1/2 | 12/33: 0.08027
2024-11-16 18:30:11 : 1/2 | 13/33: 0.08879
2024-11-16 18:30:16 : 1/2 | 14/33: 0.08187
2024-11-16 18:30:20 : 1/2 | 15/33: 0.10984
2024-11-16 18:30:24 : 1/2 | 16/33: 0.09575
2024-11-16 18:30:28 : 1/2 | 17/33: 0.09153
2024-11-16 18:30:32 : 1/2 | 18/33: 0.08654
2024-11-16 18:30:36 : 1/2 | 19/33: 0.08055
2024-11-16 18:30:40 : 1/2 | 20/33: 0.08941
2024-11-16 18:30:44 : 1/2 | 21/33: 0.07180
2024-11-16 18:30:48 : 1/2 | 22/33: 0.07075
2024-11-16 18:30:52 : 1/2 | 23/33: 0.07503
2024-11-16 18:30:57 : 1/2 | 24/33: 0.06341
2024-11-16 18:31:01 : 1/2 | 25/33: 0.06953
2024-11-16 18:31:05 : 1/2 | 26/33: 0.07931
2024-11-16 18:31:09 : 1/2 | 27/33: 0.07860
2024-11-16 18:31:13 : 1/2 | 28/33: 0.08453
2024-11-16 18:31:17 : 1/2 | 29/33: 0.11921
2024-11-16 18:31:21 : 1/2 | 30/33: 0.10498
2024-11-16 18:31:25 : 1/2 | 31/33: 0.10008
2024-11-16 18:31:29 : 1/2 | 32/33: 0.08792
2024-11-16 18:32:00 : 1: Evaluation loss | Pearson | qwk : 0.20348 | 0.69300 | 0.49500
2024-11-16 18:32:00 : Saving model p3.pt on epoch 1 (0.20348 is the new best loss!)
2024-11-16 18:32:07 : Using best model from epoch 1
2024-11-16 18:32:07 : Performing final evaluation on the evaluation set
2024-11-16 18:32:22 : Pearson: 0.69300 | QWK: 0.49500
2024-11-16 18:32:22 : Performing final evaluation on the test set
2024-11-16 18:32:38 : Pearson: 0.76200 | QWK: 0.48100
2024-11-16 18:32:38 : Losses: 94.13526916503906, 0.7540783286094666, 1.8869333267211914, 1.62418794631958, 1.4040592908859253, 1.0150988101959229, 0.7507616281509399, 0.4448547959327698, 0.3407428562641144, 0.3127223253250122, 0.3231033980846405, 0.3367968201637268, 0.3819161057472229, 0.41448381543159485, 0.42603203654289246, 0.4042150676250458, 0.4029327630996704, 0.34631583094596863, 0.3208472728729248, 0.3225342333316803, 0.25002992153167725, 0.2380715012550354, 0.2157207429409027, 0.23378190398216248, 0.2024460732936859, 0.20077982544898987, 0.20105725526809692, 0.19199499487876892, 0.2046860009431839, 0.21576733887195587, 0.22157736122608185, 0.18835122883319855, 0.2010791152715683, 0.279544472694397, 0.2959508001804352, 0.2833279073238373, 0.2665693759918213, 0.23760586977005005, 0.165570929646492, 0.13786843419075012, 0.13864651322364807, 0.11413964629173279, 0.11810765415430069, 0.0890783742070198, 0.11821839958429337, 0.08027160912752151, 0.08879144489765167, 0.08187291026115417, 0.10984237492084503, 0.09575419127941132, 0.09152592718601227, 0.08653587102890015, 0.080547034740448, 0.0894090011715889, 0.07180021703243256, 0.07074722647666931, 0.0750340074300766, 0.06341229379177094, 0.06953169405460358, 0.07930983603000641, 0.078604556620121, 0.08452918380498886, 0.11921017616987228, 0.10497903823852539, 0.10007814317941666, 0.08791658282279968
2024-11-16 18:32:38 : Fine-tuning complete on prompt 3
2024-11-16 19:46:43 : ===================== 2024-11-16T19:46:43.036058 =====================
2024-11-16 19:46:43 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 19:46:43 : Settings: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=2, sample=-1, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 19:46:45 : Train/Eval/Test: 1055/336/335
2024-11-16 19:46:45 : Started training loop for p3.pt
2024-11-16 19:46:51 : 0/2 | 0/33: 92.08424
2024-11-16 19:46:54 : 0/2 | 1/33: 0.77683
2024-11-16 19:46:56 : 0/2 | 2/33: 1.74319
2024-11-16 19:46:59 : 0/2 | 3/33: 1.79241
2024-11-16 19:47:02 : 0/2 | 4/33: 1.43988
2024-11-16 19:47:05 : 0/2 | 5/33: 1.01189
2024-11-16 19:47:08 : 0/2 | 6/33: 0.66170
2024-11-16 19:47:10 : 0/2 | 7/33: 0.42455
2024-11-16 19:47:13 : 0/2 | 8/33: 0.34756
2024-11-16 19:47:16 : 0/2 | 9/33: 0.32641
2024-11-16 19:47:19 : 0/2 | 10/33: 0.35333
2024-11-16 19:47:21 : 0/2 | 11/33: 0.37490
2024-11-16 19:47:24 : 0/2 | 12/33: 0.37757
2024-11-16 19:47:27 : 0/2 | 13/33: 0.43642
2024-11-16 19:47:30 : 0/2 | 14/33: 0.44593
2024-11-16 19:47:32 : 0/2 | 15/33: 0.40450
2024-11-16 19:47:35 : 0/2 | 16/33: 0.40424
2024-11-16 19:47:38 : 0/2 | 17/33: 0.36174
2024-11-16 19:47:40 : 0/2 | 18/33: 0.32646
2024-11-16 19:47:43 : 0/2 | 19/33: 0.31270
2024-11-16 19:47:46 : 0/2 | 20/33: 0.25632
2024-11-16 19:47:49 : 0/2 | 21/33: 0.22814
2024-11-16 19:47:51 : 0/2 | 22/33: 0.21447
2024-11-16 19:47:54 : 0/2 | 23/33: 0.20978
2024-11-16 19:47:57 : 0/2 | 24/33: 0.21489
2024-11-16 19:48:00 : 0/2 | 25/33: 0.20923
2024-11-16 19:48:02 : 0/2 | 26/33: 0.19597
2024-11-16 19:48:05 : 0/2 | 27/33: 0.19060
2024-11-16 19:48:08 : 0/2 | 28/33: 0.20956
2024-11-16 19:48:10 : 0/2 | 29/33: 0.21770
2024-11-16 19:48:13 : 0/2 | 30/33: 0.22607
2024-11-16 19:48:16 : 0/2 | 31/33: 0.21414
2024-11-16 19:48:19 : 0/2 | 32/33: 0.17771
2024-11-16 19:48:43 : 0: Evaluation loss | Pearson | qwk : 0.25856 | 0.71000 | 0.50900
2024-11-16 19:48:43 : Saving model p3.pt on epoch 0 (0.25856 is the new best loss!)
2024-11-16 19:48:47 : 1/2 | 0/33: 0.29108
2024-11-16 19:48:50 : 1/2 | 1/33: 0.29943
2024-11-16 19:48:52 : 1/2 | 2/33: 0.29819
2024-11-16 19:48:55 : 1/2 | 3/33: 0.24421
2024-11-16 19:48:58 : 1/2 | 4/33: 0.24788
2024-11-16 19:49:01 : 1/2 | 5/33: 0.18528
2024-11-16 19:49:03 : 1/2 | 6/33: 0.16942
2024-11-16 19:49:06 : 1/2 | 7/33: 0.15680
2024-11-16 19:49:09 : 1/2 | 8/33: 0.16542
2024-11-16 19:49:12 : 1/2 | 9/33: 0.15489
2024-11-16 19:49:14 : 1/2 | 10/33: 0.12762
2024-11-16 19:49:17 : 1/2 | 11/33: 0.12229
2024-11-16 19:49:20 : 1/2 | 12/33: 0.11079
2024-11-16 19:49:23 : 1/2 | 13/33: 0.10202
2024-11-16 19:49:25 : 1/2 | 14/33: 0.09389
2024-11-16 19:49:28 : 1/2 | 15/33: 0.09102
2024-11-16 19:49:31 : 1/2 | 16/33: 0.09581
2024-11-16 19:49:34 : 1/2 | 17/33: 0.09205
2024-11-16 19:49:36 : 1/2 | 18/33: 0.09983
2024-11-16 19:49:39 : 1/2 | 19/33: 0.07884
2024-11-16 19:49:42 : 1/2 | 20/33: 0.06592
2024-11-16 19:49:45 : 1/2 | 21/33: 0.09542
2024-11-16 19:49:48 : 1/2 | 22/33: 0.09329
2024-11-16 19:49:50 : 1/2 | 23/33: 0.08389
2024-11-16 19:49:53 : 1/2 | 24/33: 0.07748
2024-11-16 19:49:56 : 1/2 | 25/33: 0.06356
2024-11-16 19:49:59 : 1/2 | 26/33: 0.07487
2024-11-16 19:50:01 : 1/2 | 27/33: 0.07959
2024-11-16 19:50:04 : 1/2 | 28/33: 0.08344
2024-11-16 19:50:07 : 1/2 | 29/33: 0.10346
2024-11-16 19:50:10 : 1/2 | 30/33: 0.09628
2024-11-16 19:50:12 : 1/2 | 31/33: 0.11124
2024-11-16 19:50:15 : 1/2 | 32/33: 0.10596
2024-11-16 19:50:39 : 1: Evaluation loss | Pearson | qwk : 0.20714 | 0.69600 | 0.46400
2024-11-16 19:50:39 : Saving model p3.pt on epoch 1 (0.20714 is the new best loss!)
2024-11-16 19:50:41 : Using best model from epoch 1
2024-11-16 19:50:41 : Performing final evaluation on the evaluation set
2024-11-16 19:50:54 : Pearson: 0.69600 | QWK: 0.46400
2024-11-16 19:50:54 : Performing final evaluation on the test set
2024-11-16 19:51:07 : Pearson: 0.76200 | QWK: 0.45900
2024-11-16 19:51:07 : Losses: 92.08423614501953, 0.7768328189849854, 1.7431868314743042, 1.7924118041992188, 1.4398832321166992, 1.0118885040283203, 0.6617040038108826, 0.42454561591148376, 0.3475596606731415, 0.326410710811615, 0.35332590341567993, 0.37490469217300415, 0.3775721490383148, 0.4364224672317505, 0.4459269642829895, 0.40450188517570496, 0.40423789620399475, 0.3617415726184845, 0.3264646828174591, 0.3126966953277588, 0.25631508231163025, 0.22814321517944336, 0.2144709676504135, 0.20978312194347382, 0.2148895412683487, 0.2092278152704239, 0.1959662288427353, 0.19060494005680084, 0.2095586061477661, 0.21770060062408447, 0.22607116401195526, 0.21413610875606537, 0.1777084916830063, 0.29107722640037537, 0.2994323670864105, 0.2981918454170227, 0.2442082166671753, 0.24787814915180206, 0.18527859449386597, 0.16941691935062408, 0.15680119395256042, 0.16542352735996246, 0.1548875868320465, 0.1276235431432724, 0.12228799611330032, 0.11078999936580658, 0.10201604664325714, 0.09388984739780426, 0.09102468192577362, 0.09581241011619568, 0.09204783290624619, 0.09982825070619583, 0.07884395122528076, 0.06591511517763138, 0.09542010724544525, 0.09329186379909515, 0.08388837426900864, 0.07747938483953476, 0.06356216967105865, 0.07486580312252045, 0.07958979904651642, 0.08344164490699768, 0.10345529764890671, 0.09628481417894363, 0.11123581975698471, 0.10596216470003128
2024-11-16 19:51:07 : Fine-tuning complete on prompt 3
2024-11-16 22:53:58 : ===================== 2024-11-16T22:53:58.858454 =====================
2024-11-16 22:53:58 : ===================== FINE-TUNING ON PROMPT 3 =====================
2024-11-16 22:53:58 : Settings: Namespace(data='init/asap_5_splits.csv', bert_model_path='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p8_3', chunk_sizes='90_30_130_10', prompt=[3], batch_size=32, epochs=2, sample=-1, learning_rate=6e-05, result_file='results/finetune/p3-1.txt', save_model='../../../ix/ix_models/Multi-Scale-BERT-AES-Models/p3/p3.pt', device='cuda', log=PosixPath('logs/test2.log'))
2024-11-16 22:54:06 : Train/Eval/Test: 1055/336/335
2024-11-16 22:54:06 : Started training loop for p3.pt
2024-11-16 22:54:14 : 1/2 | 0/33: 1.23922
2024-11-16 22:54:17 : 1/2 | 1/33: 0.51666
2024-11-16 22:54:19 : 1/2 | 2/33: 0.57864
2024-11-16 22:54:22 : 1/2 | 3/33: 0.33947
2024-11-16 22:54:25 : 1/2 | 4/33: 0.28963
2024-11-16 22:54:28 : 1/2 | 5/33: 0.30318
2024-11-16 22:54:31 : 1/2 | 6/33: 0.17847
2024-11-16 22:54:34 : 1/2 | 7/33: 0.17354
2024-11-16 22:54:36 : 1/2 | 8/33: 0.17147
2024-11-16 22:54:39 : 1/2 | 9/33: 0.14436
2024-11-16 22:54:42 : 1/2 | 10/33: 0.10363
2024-11-16 22:54:45 : 1/2 | 11/33: 0.09873
2024-11-16 22:54:47 : 1/2 | 12/33: 0.07610
2024-11-16 22:54:50 : 1/2 | 13/33: 0.07343
2024-11-16 22:54:53 : 1/2 | 14/33: 0.08549
2024-11-16 22:54:56 : 1/2 | 15/33: 0.03246
2024-11-16 22:54:59 : 1/2 | 16/33: 0.07897
2024-11-16 22:55:01 : 1/2 | 17/33: 0.04384
2024-11-16 22:55:04 : 1/2 | 18/33: 0.04926
2024-11-16 22:55:07 : 1/2 | 19/33: 0.09068
2024-11-16 22:55:10 : 1/2 | 20/33: 0.06721
2024-11-16 22:55:12 : 1/2 | 21/33: 0.06843
2024-11-16 22:55:15 : 1/2 | 22/33: 0.09685
2024-11-16 22:55:18 : 1/2 | 23/33: 0.09133
2024-11-16 22:55:21 : 1/2 | 24/33: 0.06995
2024-11-16 22:55:23 : 1/2 | 25/33: 0.09065
2024-11-16 22:55:26 : 1/2 | 26/33: 0.04937
2024-11-16 22:55:29 : 1/2 | 27/33: 0.07578
2024-11-16 22:55:32 : 1/2 | 28/33: 0.07151
2024-11-16 22:55:35 : 1/2 | 29/33: 0.09528
2024-11-16 22:55:37 : 1/2 | 30/33: 0.07424
2024-11-16 22:55:40 : 1/2 | 31/33: 0.07965
2024-11-16 22:55:43 : 1/2 | 32/33: 0.07187
2024-11-16 22:56:08 : 1: Evaluation loss | Pearson | qwk : 0.23289 | 0.61200 | 0.43400
2024-11-16 22:56:08 : Saving model p3.pt on epoch 1 (0.23289 is the new best loss!)
2024-11-16 22:56:12 : 2/2 | 0/33: 0.09860
2024-11-16 22:56:15 : 2/2 | 1/33: 0.10212
2024-11-16 22:56:18 : 2/2 | 2/33: 0.04231
2024-11-16 22:56:20 : 2/2 | 3/33: 0.05922
2024-11-16 22:56:23 : 2/2 | 4/33: 0.07673
2024-11-16 22:56:26 : 2/2 | 5/33: 0.03099
2024-11-16 22:56:29 : 2/2 | 6/33: 0.04792
2024-11-16 22:56:32 : 2/2 | 7/33: 0.07798
2024-11-16 22:56:35 : 2/2 | 8/33: 0.04404
2024-11-16 22:56:37 : 2/2 | 9/33: 0.02933
2024-11-16 22:56:40 : 2/2 | 10/33: 0.02784
2024-11-16 22:56:43 : 2/2 | 11/33: 0.03697
2024-11-16 22:56:46 : 2/2 | 12/33: 0.02604
2024-11-16 22:56:48 : 2/2 | 13/33: 0.04647
2024-11-16 22:56:51 : 2/2 | 14/33: 0.04916
2024-11-16 22:56:54 : 2/2 | 15/33: 0.04425
2024-11-16 22:56:57 : 2/2 | 16/33: 0.02503
2024-11-16 22:57:00 : 2/2 | 17/33: 0.04562
2024-11-16 22:57:02 : 2/2 | 18/33: 0.04078
2024-11-16 22:57:05 : 2/2 | 19/33: 0.03371
2024-11-16 22:57:08 : 2/2 | 20/33: 0.04218
2024-11-16 22:57:11 : 2/2 | 21/33: 0.03763
2024-11-16 22:57:14 : 2/2 | 22/33: 0.03783
2024-11-16 22:57:16 : 2/2 | 23/33: 0.02469
2024-11-16 22:57:19 : 2/2 | 24/33: 0.03846
2024-11-16 22:57:22 : 2/2 | 25/33: 0.03232
2024-11-16 22:57:25 : 2/2 | 26/33: 0.02920
2024-11-16 22:57:28 : 2/2 | 27/33: 0.03039
2024-11-16 22:57:30 : 2/2 | 28/33: 0.04447
2024-11-16 22:57:33 : 2/2 | 29/33: 0.03316
2024-11-16 22:57:36 : 2/2 | 30/33: 0.04289
2024-11-16 22:57:39 : 2/2 | 31/33: 0.06022
2024-11-16 22:57:41 : 2/2 | 32/33: 0.02948
2024-11-16 22:58:05 : 2: Evaluation loss | Pearson | qwk : 0.22526 | 0.63800 | 0.38100
2024-11-16 22:58:05 : Saving model p3.pt on epoch 2 (0.22526 is the new best loss!)
2024-11-16 22:58:07 : Using best model from epoch 1
2024-11-16 22:58:07 : Performing final evaluation on the evaluation set
2024-11-16 22:58:20 : Pearson: 0.63800 | QWK: 0.38100
2024-11-16 22:58:20 : Performing final evaluation on the test set
2024-11-16 22:58:33 : Pearson: 0.71200 | QWK: 0.38400
2024-11-16 22:58:33 : Losses: 1.2392174005508423, 0.5166599154472351, 0.5786390900611877, 0.3394671082496643, 0.289628267288208, 0.3031829297542572, 0.17847372591495514, 0.17354029417037964, 0.1714656800031662, 0.14435921609401703, 0.1036258265376091, 0.09872742742300034, 0.07609749585390091, 0.07343191653490067, 0.08549477905035019, 0.032460350543260574, 0.07897160202264786, 0.043835002928972244, 0.04925943538546562, 0.09068289399147034, 0.06721317768096924, 0.06843247264623642, 0.0968463271856308, 0.0913344994187355, 0.06994543969631195, 0.09064561128616333, 0.04936889186501503, 0.07577928155660629, 0.07150942087173462, 0.0952780693769455, 0.0742448940873146, 0.07964903116226196, 0.07186807692050934, 0.09859997779130936, 0.10212384909391403, 0.042309537529945374, 0.05921994149684906, 0.07673005014657974, 0.030989747494459152, 0.0479155108332634, 0.07798438519239426, 0.044037044048309326, 0.02932772971689701, 0.027844157069921494, 0.03697461262345314, 0.026043156161904335, 0.04647110775113106, 0.04915890842676163, 0.04425358772277832, 0.0250296201556921, 0.04561964049935341, 0.04078277572989464, 0.03370952233672142, 0.04217991605401039, 0.03762677311897278, 0.037830375134944916, 0.02469347044825554, 0.03846263512969017, 0.032323550432920456, 0.02919694036245346, 0.030390001833438873, 0.04447495564818382, 0.033160239458084106, 0.042885273694992065, 0.060221876949071884, 0.02947787009179592
2024-11-16 22:58:33 : Fine-tuning complete on prompt 3
